{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leilayfarsani/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('IATI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please make sure it is set in the .env file or update it if necessary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activity/apache/select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def fetch_page(start):\n",
    "    params = {\n",
    "        'q': '(sector_code:(11250 OR 12240 OR 31110 OR 31120 OR 31130 OR 31140 OR 31150 OR 31161 OR 31162 OR 31163 OR 31164 OR 31165 OR 31166 OR 31181 OR 31182 OR 31191 OR 31192 OR 31193 OR 31194 OR 31195 OR 31210 OR 31220 OR 31261 OR 31281 OR 31282 OR 31291 OR 31310 OR 31320 OR 31381 OR 31382 OR 31391 OR 32161 OR 32162 OR 43040 OR 43071 OR 43072 OR 43073 OR 52010) OR sector_vocabulary:2 AND sector_code:(311 OR 312 OR 313)) OR (title_narrative:(\"food security\" OR \"food insecurity\") OR description_narrative:(\"food security\" OR \"food insecurity\"))',\n",
    "        #'fl': 'iati_identifier,title_narrative,description_narrative,sector_code,activity_date_iso_date,activity_date_type,recipient_country_code',\n",
    "        'fq': 'activity_date_type:2 AND activity_date_iso_date:[2021-01-01T00:00:00Z TO *]',  \n",
    "        'rows': 1000,\n",
    "        'start': start\n",
    "    }\n",
    "    headers = {'Ocp-Apim-Subscription-Key': api_key}\n",
    "    \n",
    "    for attempt in range(5):  \n",
    "        try:\n",
    "            response = requests_retry_session().get(base_url, headers=headers, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            docs = response.json()['response']['docs']\n",
    "            return docs, len(docs)\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                wait = 2 ** attempt  # exponential backoff\n",
    "                print(f\"Rate limit hit. Waiting for {wait} seconds.\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"HTTP error occurred: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    print(f\"Failed to fetch data for start={start} after 5 attempts\")\n",
    "    return [], 0\n",
    "\n",
    "def get_total_results():\n",
    "    params = {\n",
    "        'q': '(sector_code:(11250 OR 12240 OR 31110 OR 31120 OR 31130 OR 31140 OR 31150 OR 31161 OR 31162 OR 31163 OR 31164 OR 31165 OR 31166 OR 31181 OR 31182 OR 31191 OR 31192 OR 31193 OR 31194 OR 31195 OR 31210 OR 31220 OR 31261 OR 31281 OR 31282 OR 31291 OR 31310 OR 31320 OR 31381 OR 31382 OR 31391 OR 32161 OR 32162 OR 43040 OR 43071 OR 43072 OR 43073 OR 52010) OR sector_vocabulary:2 AND sector_code:(311 OR 312 OR 313)) OR (title_narrative:(\"food security\" OR \"food insecurity\") OR description_narrative:(\"food security\" OR \"food insecurity\"))',\n",
    "        'rows': 0\n",
    "    }\n",
    "    headers = {'Ocp-Apim-Subscription-Key': api_key}\n",
    "    response = requests_retry_session().get(base_url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['response']['numFound']\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return 0\n",
    "\n",
    "base_url = \"https://api.iatistandard.org/datastore/activity/select\"\n",
    "total_results = get_total_results() \n",
    "all_activities = []\n",
    "page_sizes = []\n",
    "\n",
    "print(f\"Total results to fetch: {total_results}\")\n",
    "\n",
    "max_empty_pages = 5  \n",
    "empty_page_count = 0\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    futures = [executor.submit(fetch_page, i) for i in range(0, total_results, 1000)]\n",
    "    for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "        docs, size = future.result()\n",
    "        all_activities.extend(docs)\n",
    "        page_sizes.append(size)\n",
    "        print(f\"Fetched page {i+1}/{len(futures)} with {size} documents\")\n",
    "        \n",
    "        if size == 0:\n",
    "            empty_page_count += 1\n",
    "            if empty_page_count >= max_empty_pages:\n",
    "                print(f\"Stopped fetching after {max_empty_pages} consecutive empty pages\")\n",
    "                break\n",
    "        else:\n",
    "            empty_page_count = 0\n",
    "\n",
    "        time.sleep(1)  # Adding a small delay between requests\n",
    "\n",
    "print(f\"Page sizes: {page_sizes}\")\n",
    "print(f\"Sum of page sizes: {sum(page_sizes)}\")\n",
    "print(f\"Total activities fetched: {len(all_activities)}\")\n",
    "\n",
    "df_activities = pd.DataFrame(all_activities)\n",
    "\n",
    "def clean_list_field(field):\n",
    "    return field[0] if isinstance(field, list) and len(field) > 0 else field\n",
    "\n",
    "for col in ['sector_code', 'title_narrative', 'description_narrative', 'recipient_country_code']:\n",
    "    df_activities[col] = df_activities[col].apply(clean_list_field)\n",
    "\n",
    "df_activities['start_date'] = pd.to_datetime(df_activities['activity_date_iso_date'].apply(clean_list_field), errors='coerce')\n",
    "\n",
    "print(df_activities.head())\n",
    "print(f\"Shape of DataFrame: {df_activities.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activities = df_activities.drop_duplicates(subset='iati_identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to iati_activities_20240905_223018.xlsx\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # Convert all datetime columns to timezone-naive\n",
    "# for col in df_activities.select_dtypes(include=['datetime64', 'datetimetz']).columns:\n",
    "#     df_activities[col] = df_activities[col].dt.tz_localize(None)\n",
    "\n",
    "# # Generate a timestamp for the filename\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# filename = f\"iati_activities_{timestamp}.xlsx\"\n",
    "\n",
    "# # Save to Excel\n",
    "# df_activities.to_excel(filename, index=False, engine='openpyxl')\n",
    "\n",
    "# print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 1000)  \n",
    "# print(df_activities_unique['sector_code'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Validation:\n",
    "Performing some additional checks on the data to ensure its quality and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sector_code\n",
      "31120       640\n",
      "6           622\n",
      "72010       578\n",
      "3           552\n",
      "311         403\n",
      "           ... \n",
      "31281         1\n",
      "34-01-01      1\n",
      "34-01-07      1\n",
      "11430         1\n",
      "BH            1\n",
      "Name: count, Length: 253, dtype: int64\n",
      "recipient_country_code\n",
      "AF    430\n",
      "ET    422\n",
      "SS    337\n",
      "SO    331\n",
      "YE    272\n",
      "     ... \n",
      "GZ      1\n",
      "CW      1\n",
      "CA      1\n",
      "ES      1\n",
      "NR      1\n",
      "Name: count, Length: 174, dtype: int64\n",
      "1979-01-01 00:00:00+00:00 2028-12-31 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "print(df_activities['sector_code'].value_counts())\n",
    "print(df_activities['recipient_country_code'].value_counts())\n",
    "print(df_activities['start_date'].min(), df_activities['start_date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of activities with start date before 2021-01-01: 4415\n",
      "                               iati_identifier                start_date  \\\n",
      "2                          GB-CHC-220949-P8501 2020-06-20 00:00:00+00:00   \n",
      "10                         XM-DAC-41301-663721 2020-03-01 00:00:00+00:00   \n",
      "12                     XM-DAC-3-1-264893-32579 2018-01-29 00:00:00+00:00   \n",
      "14            XI-IATI-EC_INTPA-2020-PCC-412348 2020-11-27 00:00:00+00:00   \n",
      "25  XM-OCHA-CBPF-AFG-19/3481/RA4/FSAC/UN/14864 2020-01-01 00:00:00+00:00   \n",
      "\n",
      "                               activity_date_iso_date  \n",
      "2        [2020-06-20T00:00:00Z, 2021-06-14T00:00:00Z]  \n",
      "10       [2020-03-01T00:00:00Z, 2023-02-28T00:00:00Z]  \n",
      "12  [2018-01-29T00:00:00Z, 2020-09-23T00:00:00Z, 2...  \n",
      "14  [2020-11-27T00:00:00Z, 2020-11-27T00:00:00Z, 2...  \n",
      "25  [2020-01-01T00:00:00Z, 2020-01-01T00:00:00Z, 2...  \n"
     ]
    }
   ],
   "source": [
    "early_dates = df_activities[df_activities['start_date'] < '2021-01-01']\n",
    "print(f\"Number of activities with start date before 2021-01-01: {len(early_dates)}\")\n",
    "print(early_dates[['iati_identifier', 'start_date', 'activity_date_iso_date']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               iati_identifier                start_date  \\\n",
      "2                          GB-CHC-220949-P8501 2020-06-20 00:00:00+00:00   \n",
      "10                         XM-DAC-41301-663721 2020-03-01 00:00:00+00:00   \n",
      "12                     XM-DAC-3-1-264893-32579 2018-01-29 00:00:00+00:00   \n",
      "14            XI-IATI-EC_INTPA-2020-PCC-412348 2020-11-27 00:00:00+00:00   \n",
      "25  XM-OCHA-CBPF-AFG-19/3481/RA4/FSAC/UN/14864 2020-01-01 00:00:00+00:00   \n",
      "\n",
      "                                         parsed_dates  \n",
      "2        [2020-06-20T00:00:00Z, 2021-06-14T00:00:00Z]  \n",
      "10       [2020-03-01T00:00:00Z, 2023-02-28T00:00:00Z]  \n",
      "12  [2018-01-29T00:00:00Z, 2020-09-23T00:00:00Z, 2...  \n",
      "14  [2020-11-27T00:00:00Z, 2020-11-27T00:00:00Z, 2...  \n",
      "25  [2020-01-01T00:00:00Z, 2020-01-01T00:00:00Z, 2...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/bbgshr8n1m3fx_npbqvt9_w00000gn/T/ipykernel_57858/2312078001.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  early_dates['parsed_dates'] = early_dates['activity_date_iso_date'].apply(parse_dates)\n"
     ]
    }
   ],
   "source": [
    "def parse_dates(date_list):\n",
    "    return [date for date in date_list if date.startswith('2')]  \n",
    "\n",
    "early_dates['parsed_dates'] = early_dates['activity_date_iso_date'].apply(parse_dates)\n",
    "print(early_dates[['iati_identifier', 'start_date', 'parsed_dates']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 start_date             earliest_date\n",
      "0 2027-02-01 00:00:00+00:00 2022-06-21 00:00:00+00:00\n",
      "1 2022-05-08 00:00:00+00:00 2022-05-08 00:00:00+00:00\n",
      "2 2020-06-20 00:00:00+00:00 2020-06-20 00:00:00+00:00\n",
      "3 2024-02-05 00:00:00+00:00 2024-02-05 00:00:00+00:00\n",
      "4 2023-12-01 00:00:00+00:00 2023-12-01 00:00:00+00:00\n",
      "Number of activities where earliest_date != start_date: 721\n"
     ]
    }
   ],
   "source": [
    "def get_earliest_date(date_list):\n",
    "    return min(parse_dates(date_list), default=None)\n",
    "\n",
    "df_activities['earliest_date'] = df_activities['activity_date_iso_date'].apply(get_earliest_date)\n",
    "df_activities['earliest_date'] = pd.to_datetime(df_activities['earliest_date'])\n",
    "\n",
    "print(df_activities[['start_date', 'earliest_date']].head())\n",
    "print(f\"Number of activities where earliest_date != start_date: {(df_activities['earliest_date'] != df_activities['start_date']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions/Apache-Solr default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Transactions in XML format is not accessible from Datastore API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(retries=3, backoff_factor=0.3, status_forcelist=(500, 502, 504), session=None):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def fetch_page(start):\n",
    "    params = {\n",
    "        'q': '(sector_code:(11250 OR 12240 OR 31110 OR 31120 OR 31130 OR 31140 OR 31150 OR 31161 OR 31162 OR 31163 OR 31164 OR 31165 OR 31166 OR 31181 OR 31182 OR 31191 OR 31192 OR 31193 OR 31194 OR 31195 OR 31210 OR 31220 OR 31261 OR 31281 OR 31282 OR 31291 OR 31310 OR 31320 OR 31381 OR 31382 OR 31391 OR 32161 OR 32162 OR 43040 OR 43071 OR 43072 OR 43073 OR 52010) OR sector_vocabulary:2 AND sector_code:(311 OR 312 OR 313)) OR (description_narrative:(\"food security\" OR \"food insecurity\"))',\n",
    "        #'fl': 'iati_identifier,transaction_value,transaction_date_iso_date,sector_code,recipient_country_code',\n",
    "        'fq': 'transaction_transaction_date_iso_date:[2021-01-01T00:00:00Z TO *]', \n",
    "        'rows': 1000,\n",
    "        'start': start\n",
    "    }\n",
    "    headers = {'Ocp-Apim-Subscription-Key': api_key}\n",
    "    \n",
    "    for attempt in range(5):  \n",
    "        try:\n",
    "            response = requests_retry_session().get(base_url, headers=headers, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            docs = response.json()['response']['docs']\n",
    "            return docs, len(docs)\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                wait = 2 ** attempt  # exponential backoff\n",
    "                print(f\"Rate limit hit. Waiting for {wait} seconds.\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"HTTP error occurred: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    print(f\"Failed to fetch data for start={start} after 5 attempts\")\n",
    "    return [], 0\n",
    "\n",
    "def get_total_results():\n",
    "    params = {\n",
    "        'q': '(sector_code:(11250 OR 12240 OR 31110 OR 31120 OR 31130 OR 31140 OR 31150 OR 31161 OR 31162 OR 31163 OR 31164 OR 31165 OR 31166 OR 31181 OR 31182 OR 31191 OR 31192 OR 31193 OR 31194 OR 31195 OR 31210 OR 31220 OR 31261 OR 31281 OR 31282 OR 31291 OR 31310 OR 31320 OR 31381 OR 31382 OR 31391 OR 32161 OR 32162 OR 43040 OR 43071 OR 43072 OR 43073 OR 52010) OR sector_vocabulary:2 AND sector_code:(311 OR 312 OR 313)) OR (description_narrative:(\"food security\" OR \"food insecurity\"))',\n",
    "        'rows': 0\n",
    "    }\n",
    "    headers = {'Ocp-Apim-Subscription-Key': api_key}\n",
    "    response = requests_retry_session().get(base_url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['response']['numFound']\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return 0\n",
    "\n",
    "# base URL for transaction collection\n",
    "base_url = \"https://api.iatistandard.org/datastore/transaction/select\"\n",
    "\n",
    "total_results = get_total_results()\n",
    "all_transactions = []\n",
    "page_sizes = []\n",
    "\n",
    "print(f\"Total results to fetch: {total_results}\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor: \n",
    "    futures = [executor.submit(fetch_page, i) for i in range(0, total_results, 1000)]\n",
    "    for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "        docs, size = future.result()\n",
    "        all_transactions.extend(docs)\n",
    "        page_sizes.append(size)\n",
    "        print(f\"Fetched page {i+1}/{len(futures)} with {size} documents\")\n",
    "        time.sleep(1) \n",
    "\n",
    "print(f\"Page sizes: {page_sizes}\")\n",
    "print(f\"Sum of page sizes: {sum(page_sizes)}\")\n",
    "print(f\"Total transactions fetched: {len(all_transactions)}\")\n",
    "\n",
    "df_transactions = pd.DataFrame(all_transactions)\n",
    "print(df_transactions.head())\n",
    "print(f\"Shape of DataFrame: {df_transactions.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique transactions: 9138\n"
     ]
    }
   ],
   "source": [
    "df_transactions = df_transactions.drop_duplicates(subset='iati_identifier')\n",
    "print(f\"Total unique transactions: {len(df_transactions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to iati_transactions_20240905_224122.xlsx\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_df_to_excel(df, filename):\n",
    "    # Converting all datetime columns to timezone-naive\n",
    "    for col in df.select_dtypes(include=['datetime64', 'datetimetz']).columns:\n",
    "        df[col] = pd.to_datetime(df[col], utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # Generating a timestamp for the filename if not provided\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"data_{timestamp}.xlsx\"\n",
    "\n",
    "    # Save to Excel\n",
    "    try:\n",
    "        df.to_excel(filename, index=False, engine='openpyxl')\n",
    "        print(f\"Data successfully saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the file: {e}\")\n",
    "        print(\"Attempting to save as CSV instead...\")\n",
    "        csv_filename = filename.rsplit('.', 1)[0] + '.csv'\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        print(f\"Data saved as CSV to {csv_filename}\")\n",
    "\n",
    "\n",
    "#save_df_to_excel(df_transactions, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
